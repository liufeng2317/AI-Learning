{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import sys \n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "sys.path.append(os.path.abspath(r\"../..\"))\n",
    "from model.generator import Generator, Generator_Transpose\n",
    "from model.discriminator import DiscriminatorResnet, DiscriminatorLinear, DiscriminatorConv\n",
    "from utils_.utils import weights_init, weight_init\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = [3, 32, 32]\n",
    "batch_size = 128\n",
    "Epoch = 1000\n",
    "GenEpoch = 1\n",
    "in_channel = 64"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Result save path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_now = time.strftime('%Y-%m-%d-%H_%M_%S', time.localtime(time.time()))\n",
    "log_path = f'./log/{time_now}'\n",
    "os.makedirs(log_path)\n",
    "os.makedirs(f'{log_path}/image')\n",
    "os.makedirs(f'{log_path}/image/image_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'using device: {device}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集\n",
    "cifar_train = datasets.CIFAR10('../../../../data/cifar', True,\n",
    "                                transform=transforms.Compose([\n",
    "                                transforms.Resize((32, 32)),\n",
    "                                transforms.ToTensor()]\n",
    "                                ),\n",
    "                                download=True)\n",
    "cifar_train = DataLoader(cifar_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "cifar_test = datasets.CIFAR10('../../../../data/cifar', False,\n",
    "                                transform=transforms.Compose([\n",
    "                                transforms.Resize((32, 32)),\n",
    "                                transforms.ToTensor()]\n",
    "                                ),\n",
    "                                download=True)\n",
    "cifar_test = DataLoader(cifar_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# check the shape of data\n",
    "for data in cifar_train:\n",
    "    images,targets = data\n",
    "    print(images.shape,targets.shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Generator_Transpose(\n",
       "   (up1): Sequential(\n",
       "     (0): ConvTranspose2d(64, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "     (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): ReLU(inplace=True)\n",
       "   )\n",
       "   (up2): Sequential(\n",
       "     (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "     (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): ReLU(inplace=True)\n",
       "   )\n",
       "   (up3): Sequential(\n",
       "     (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "     (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): ReLU(inplace=True)\n",
       "   )\n",
       "   (up4): Sequential(\n",
       "     (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), bias=False)\n",
       "     (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): ReLU(inplace=True)\n",
       "   )\n",
       "   (out_layer): Sequential(\n",
       "     (0): ConvTranspose2d(32, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): Sigmoid()\n",
       "   )\n",
       " ),\n",
       " DiscriminatorConv(\n",
       "   (conv_layer): Sequential(\n",
       "     (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "     (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "     (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "     (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "     (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "     (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "     (9): Conv2d(256, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "     (10): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (12): Sigmoid()\n",
       "   )\n",
       " ))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator_Transpose(in_channel=in_channel)\n",
    "dis = DiscriminatorConv(input_size=input_size)\n",
    "gen.apply(weight_init)\n",
    "dis.apply(weight_init)\n",
    "gen.to(device)\n",
    "dis.to(device)\n",
    "gen,dis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:  32%|███▏      | 125/391 [00:08<00:17, 14.97it/s, dis-loss=0.61, gen-loss=0.748] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39m# 获取标注向量\u001b[39;00m\n\u001b[1;32m     20\u001b[0m valid \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(img\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint64)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 21\u001b[0m fake \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(img\u001b[39m.\u001b[39;49msize()[\u001b[39m0\u001b[39;49m], dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mint64)\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     22\u001b[0m \u001b[39m# 随机生成一组数据\u001b[39;00m\n\u001b[1;32m     23\u001b[0m G_img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn([img\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m], in_channel, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m], requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 设置损失函数\n",
    "Loss = nn.CrossEntropyLoss()\n",
    "# 设置优化器\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "opt_dis = optim.Adam(dis.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "# 模型训练\n",
    "gen.train()\n",
    "dis.train()\n",
    "gen_loss_list = []       # 生成网路损失\n",
    "dis_loss_list = []       # 判别网络损失\n",
    "for epoch in range(Epoch):\n",
    "    with tqdm(total=cifar_train.__len__(), desc=f'Epoch {epoch + 1}/{Epoch}') as pbar:\n",
    "        gen_loss_avg = []\n",
    "        dis_loss_avg = []\n",
    "        index = 0       # 记录训练了多少个batch\n",
    "        for batchidx, (img, _) in enumerate(cifar_train):\n",
    "            # 获取数据\n",
    "            img = img.to(device)\n",
    "            # 获取标注向量\n",
    "            valid = torch.ones(img.size()[0], dtype=torch.int64).to(device)\n",
    "            fake = torch.zeros(img.size()[0], dtype=torch.int64).to(device)\n",
    "            # 随机生成一组数据\n",
    "            G_img = torch.randn([img.size()[0], in_channel, 1, 1], requires_grad=True).to(device)\n",
    "            # ------------------更新判别器------------------\n",
    "            # 前向计算\n",
    "            G_pred_gen = gen(G_img)\n",
    "            G_pred_dis = dis(G_pred_gen.detach())\n",
    "            R_pred_dis = dis(img)\n",
    "            # 计算损失\n",
    "            G_loss = Loss(G_pred_dis, fake)\n",
    "            R_loss = Loss(R_pred_dis, valid)\n",
    "            dis_loss = (R_loss + G_loss) / 2\n",
    "            dis_loss_avg.append(dis_loss.item())\n",
    "            # 反向传播\n",
    "            opt_dis.zero_grad()\n",
    "            dis_loss.backward()\n",
    "            opt_dis.step()\n",
    "            # ------------------更新生成器------------------\n",
    "            # 前向计算\n",
    "            G_pred_gen = gen(G_img)\n",
    "            G_pred_dis = dis(G_pred_gen)\n",
    "            # 计算损失\n",
    "            gen_loss = Loss(G_pred_dis, valid)\n",
    "            gen_loss_avg.append(gen_loss.item())\n",
    "            # 反向传播\n",
    "            opt_gen.zero_grad()\n",
    "            gen_loss.backward()\n",
    "            opt_gen.step()\n",
    "            # 保存过程图片\n",
    "            if index % 100 == 0 or index + 1 == cifar_train.__len__():\n",
    "                save_image(G_pred_gen, f'{log_path}/image/image_all/epoch-{epoch}-index-{index}.png')\n",
    "            index += 1\n",
    "            # ------------------进度条更新------------------\n",
    "            pbar.set_postfix(**{\n",
    "                'gen-loss': sum(gen_loss_avg) / len(gen_loss_avg),\n",
    "                'dis-loss': sum(dis_loss_avg) / len(dis_loss_avg)\n",
    "            })\n",
    "            pbar.update(1)\n",
    "    save_image(G_pred_gen, f'{log_path}/image/epoch-{epoch}.png')\n",
    "    filename = 'epoch%d-genLoss%.2f-disLoss%.2f' % (epoch, sum(gen_loss_avg) / len(gen_loss_avg), sum(dis_loss_avg) / len(dis_loss_avg))\n",
    "    torch.save(gen.state_dict(), f'{log_path}/{filename}-gen.pth')\n",
    "    torch.save(dis.state_dict(), f'{log_path}/{filename}-dis.pth')\n",
    "    # 记录损失\n",
    "    gen_loss_list.append(sum(gen_loss_avg) / len(gen_loss_avg))\n",
    "    dis_loss_list.append(sum(dis_loss_avg) / len(dis_loss_avg))\n",
    "    # 绘制损失图像并保存\n",
    "    plt.figure(0)\n",
    "    plt.plot(range(epoch + 1), gen_loss_list, 'r--', label='gen loss')\n",
    "    plt.plot(range(epoch + 1), dis_loss_list, 'r--', label='dis loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.savefig(f'{log_path}/loss.png', dpi=300)\n",
    "    plt.close(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../log/2023-02-11-17_52_12/epoch999-genLoss1.21-disLoss0.40-gen.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m gen \u001b[39m=\u001b[39m Generator_Transpose(in_channel\u001b[39m=\u001b[39min_channel)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m dis \u001b[39m=\u001b[39m DiscriminatorLinear(input_size\u001b[39m=\u001b[39minput_size)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m gen\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(gen_para_path, map_location\u001b[39m=\u001b[39;49mdevice))\n\u001b[1;32m      9\u001b[0m gen\u001b[39m.\u001b[39meval()\n\u001b[1;32m     10\u001b[0m \u001b[39m# 随机生成一组数据\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    231\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../log/2023-02-11-17_52_12/epoch999-genLoss1.21-disLoss0.40-gen.pth'"
     ]
    }
   ],
   "source": [
    "input_size = [3, 32, 32]\n",
    "in_channel = 64\n",
    "gen_para_path = '../../log/2023-02-11-17_52_12/epoch999-genLoss1.21-disLoss0.40-gen.pth'\n",
    "dis_para_path = '../../log/2023-02-11-17_52_12/epoch999-genLoss1.21-disLoss0.40-dis.pth'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "gen = Generator_Transpose(in_channel=in_channel).to(device)\n",
    "dis = DiscriminatorLinear(input_size=input_size).to(device)\n",
    "gen.load_state_dict(torch.load(gen_para_path, map_location=device))\n",
    "gen.eval()\n",
    "# 随机生成一组数据\n",
    "G_img = torch.randn([1, in_channel, 1, 1], requires_grad=False).to(device)\n",
    "# 放入网路\n",
    "G_pred = gen(G_img)\n",
    "G_dis = dis(G_pred)\n",
    "print('generator-dis:', G_dis)\n",
    "# 图像显示\n",
    "G_pred = G_pred[0, ...]\n",
    "G_pred = G_pred.detach().cpu().numpy()\n",
    "G_pred = np.array(G_pred * 255)\n",
    "G_pred = np.transpose(G_pred, [1, 2, 0])\n",
    "G_pred = Image.fromarray(np.uint8(G_pred))\n",
    "G_pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "971ee1da7255ecde3216791eeee73f2ee8d2956ba6b6da13cdfb2a534ec0c35f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
