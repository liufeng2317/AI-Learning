<!--
 * @Author: LiuFeng(USTC) : liufeng2317@mail.ustc.edu.cn
 * @Date: 2023-06-19 20:55:05
 * @LastEditors: LiuFeng
 * @LastEditTime: 2023-06-19 22:02:46
 * @FilePath: /Transformer/002_transformer模型结构.md
 * @Description: 
 * Copyright (c) 2023 by ${git_name} email: ${git_email}, All Rights Reserved.
-->

- [1. 整体结构](#1-整体结构)
  - [1.1 步骤1：Embedding](#11-步骤1embedding)
  - [1.2 Encoder](#12-encoder)
  - [1.3 Decoder](#13-decoder)
- [2. 细节说明](#2-细节说明)
  - [2.1 Transformer输入](#21-transformer输入)
    - [(1) 单词Embedding](#1-单词embedding)
    - [(2) 位置Embedding](#2-位置embedding)
  - [2.2 自注意力机制(Self-Attention)](#22-自注意力机制self-attention)
    - [(1) Self-Attention结构](#1-self-attention结构)
    - [(2) Q、K、V的计算](#2-qkv的计算)
    - [(3) Self-Attention 的输出](#3-self-attention-的输出)
    - [(4) Multi-Head Attention](#4-multi-head-attention)
  - [Encoder结构](#encoder结构)
    - [(1) Add \& Norm](#1-add--norm)
    - [(2) Feed Forward](#2-feed-forward)
    - [(3) 组成Encoder](#3-组成encoder)
  - [Decoder结构](#decoder结构)


## 1. 整体结构

![](Md_img/2023-06-19-20-55-35.png)

* Encoder
* Decoder都包含6个block

### 1.1 步骤1：Embedding
将输入句子的每一个单词用向量X表示（词Embedding + 位置Embedding）
![](Md_img/2023-06-19-20-57-53.png)

### 1.2 Encoder
将单词得到的词向量矩阵传入Encoder中，经过6个Encoder Block 可以得到句子所有单词的编码信息矩阵C

![](Md_img/2023-06-19-21-01-41.png)

### 1.3 Decoder
将Encoder输出的编码信息矩阵C传递到Decoder之中，Decoder会根据当前翻译过的单词 1~i，翻译下一个单词i+1. 翻译过程中单词 i+1需要通过 **Mask** 掩盖

![](Md_img/2023-06-19-21-03-30.png)

## 2. 细节说明
### 2.1 Transformer输入
Transformer中单词的输入表示 x由**单词Embedding**和**位置Embedding**相加得到

![](Md_img/2023-06-19-21-06-01.png)

#### (1) 单词Embedding
（1）Word2Vec
（2）Glove
（3）可以再Transformer中训练得到

#### (2) 位置Embedding
Transformer不采用RNN的结构，使用全局的结构信息，不能利用单词的顺序信息（对于NLP非常重要）。所以Transformer中使用位置Embedding保存单词在序列中的绝对或者相对位置。

位置embedding用 PE表示，其维度和单词的Embedding一致。PE可以通过训练得到，也可以使用某种公式计算得到。例如

$$PE_{(pos,2i)} = sin(pos/10000^{2i/d})$$

$$PE_{(pos,2i+1)} = cos(pos/10000^{2i/d})$$

其中，pos表示单词在句子中的位置。d表示PE的维度（与词Embedding一致），2i表示偶数的维度，2i+1表示奇数维度（即2i<=d,2i+1<=d）。使用这种公式计算PE下面的好处：

* 使PE能够适应比训练集里面所有句子更长的句子，假设训练集里面最长的句子是有20个单词，突然来了一个长度为21的单词，使用公式计算的方法可以计算出第21位的 position embedding
* 可以让模型更容易地计算出相对位置，对于固定长度的间距K，PE(pos+k)可以用PE（pos）计算得到。
  * $$Sin(A+B) = Sin(A)cos(B) + Cos(A)Sin(B)$$
  * $$Cos(A+B) = Cos(A)Cos(B) - Sin(A)Sin(B)$$


### 2.2 自注意力机制(Self-Attention)
![](Md_img/2023-06-19-21-17-36.png)

Transformer结构
* 左侧是 Encoder block
* 右侧为 Decoder block.
* 红色圈中的部分为 Multi-Head Attention，由多个 self-attention 组成

可以看到Encoder Block包含一个**Multi-Head attention**，而 Decoder block包含两个 Multi-Head Attention（其中一个用到Masked）。Multi-Head Attention 上方还包含一个 **Add&Norm** 层，**Add**表示残差连接用于防止网络退化，**Norm** 表示Layer Normalization，用于对每一层的激活值进行归一化。

#### (1) Self-Attention结构
![](Md_img/2023-06-19-21-23-59.png)

计算的时候需要用到 矩阵**Q(query查询)**、**K(key 键)**、**V(value值)**

实际之中，Self-Attention接收的是输入（单词的表示向量x组成的矩阵X）或者上一个Encoder block的输出。而Q、K、V正是通过self-attention 的输入进行线性变换得到的。

#### (2) Q、K、V的计算
Self-Attention 的输入用矩阵X进行表示，可以使用线性变化矩阵WQ、WK、WV计算得到Q、K、V。计算如下图表示

![](Md_img/2023-06-19-21-28-40.png)


#### (3) Self-Attention 的输出
得到矩阵Q、K、V之后就可以计算出 Self-Attention的输出了，计算的公式如下
$$Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}}) V$$

公式中计算矩阵Q和K每一行向量的内积，为了防止内积过大，因此除以 $d_k$的平方根。Q乘以K的转置后，得到的矩阵行列数都为n，n为句子单词数，这个矩阵可以表示单词之间的attention强度。下图为Q乘以$K^T$,1234表示的是句子中的单词。

![](Md_img/2023-06-19-21-35-16.png)

得到$QK^T$之后，使用**Softmax**计算每一个单词对于其他单词的attention系数，公式中的**Softmax**是对矩阵的每一行进行**Softmax**，即每一行的和都变为1.

![](Md_img/2023-06-19-21-37-32.png)

得到**Softmax**矩阵之后可以和V相乘，得到最终的输出Z
![](Md_img/2023-06-19-21-37-57.png)

上图中 **Softmax**矩阵的第一行表示单词1与其他所有单词的 **Attention** 系数，最终单词1的输出$Z_1$等于所有单词i的值$V_i$根据 **Attention** 系数的比例加在一起得到，如下图表示：

![](Md_img/2023-06-19-21-43-37.png)

#### (4) Multi-Head Attention
通过self-attention可以得到输出矩阵Z，而multi-head attention就是由多个self-attention组合得到的，下图是multi-head attention结构图

![](Md_img/2023-06-19-21-44-58.png)

multi-head attention包含多个self-attention层，首先将输入X分别传输到 h个不同的self-attention之中，计算得到z个输出矩阵z。下面是h=8的输出矩阵

![](Md_img/2023-06-19-21-47-44.png)

得到输出矩阵$Z_1$到$Z_8$之后，Multi-Head Attention将其拼接到一起（Concate），然后传入到一个Linear层之中，得到$Multi-Head Attention$最终的输出Z

![](Md_img/2023-06-19-21-48-55.png)

Multi-head attention之后的输出矩阵Z和输入的矩阵X的维度是相同的

### Encoder结构

#### (1) Add & Norm
add && Norm顾名思义是由两个部分组成的，计算公式如下
$$LayerNorm(X + MultiHeadAttention(X))$$
$$LayerNorm(X+FeedForward(X))$$

其中X表示Multihead Attention或者Feed Forward的输入，MultiHeadAttention(X)和FeedForward(X)表示输出（输出和输入X维度一样，可以相加）

**Add**指的是 $X + MultiHeadAttention(X)$，是一种残差连接，通用用于解决多层网络训练的问题，可以让网络只关心当前差异的部分，在ResNet中经常用到

![](Md_img/2023-06-19-21-53-59.png)

**Norm**指的是Layer Normalization，通常用于RNN结构之中，Layer Normalization 会将每一层神经元的输入都转换成均值和方差都相同的，加快收敛。

#### (2) Feed Forward
Feed Forward层是一个两层的全连接层，第一层的激活函数为ReLU,第二层不使用激活函数，对应公式为
$$max(0,XW_1 + b_1)W_2 + b_2$$

X是输入，Feed Forward最终得到的输出矩阵的维度与X一致。

#### (3) 组成Encoder
通过上面的 Multi-Head attention 、 Feed Forward、Add&Norm就可以构造出一个Encoder block，Encoder Block接收输入矩阵 $X_{n \times d}$，得到一个输出矩阵$O_{n \times d}$.多个 Encoder Block组合叠加就得到了Encoder。

第一个 Encoder block 的输入为句子单词的表示向量矩阵，后续 Encoder block 的输入是前一个 Encoder block 的输出，最后一个 Encoder block 输出的矩阵就是编码信息矩阵 C，这一矩阵后续会用到 Decoder 中。

![](Md_img/2023-06-19-22-02-14.png)


### Decoder结构

