<!--
 * @Author: LiuFeng(USTC) : liufeng2317@mail.ustc.edu.cn
 * @Date: 2023-06-19 20:55:05
 * @LastEditors: LiuFeng
 * @LastEditTime: 2023-06-19 21:18:07
 * @FilePath: /Transformer/002_transformer模型结构.md
 * @Description: 
 * Copyright (c) 2023 by ${git_name} email: ${git_email}, All Rights Reserved.
-->
## 1. 整体结构

![](Md_img/2023-06-19-20-55-35.png)

* Encoder
* Decoder都包含6个block

### 1.1 步骤1：Embedding
将输入句子的每一个单词用向量X表示（词Embedding + 位置Embedding）
![](Md_img/2023-06-19-20-57-53.png)

### 1.2 Encoder
将单词得到的词向量矩阵传入Encoder中，经过6个Encoder Block 可以得到句子所有单词的编码信息矩阵C

![](Md_img/2023-06-19-21-01-41.png)

### 1.3 Decoder
将Encoder输出的编码信息矩阵C传递到Decoder之中，Decoder会根据当前翻译过的单词 1~i，翻译下一个单词i+1. 翻译过程中单词 i+1需要通过 **Mask** 掩盖

![](Md_img/2023-06-19-21-03-30.png)

## 2. 细节说明
### 2.1 Transformer输入
Transformer中单词的输入表示 x由**单词Embedding**和**位置Embedding**相加得到

![](Md_img/2023-06-19-21-06-01.png)

#### (1) 单词Embedding
（1）Word2Vec
（2）Glove
（3）可以再Transformer中训练得到

#### (2) 位置Embedding
Transformer不采用RNN的结构，使用全局的结构信息，不能利用单词的顺序信息（对于NLP非常重要）。所以Transformer中使用位置Embedding保存单词在序列中的绝对或者相对位置。

位置embedding用 PE表示，其维度和单词的Embedding一致。PE可以通过训练得到，也可以使用某种公式计算得到。例如

$$PE_{(pos,2i)} = sin(pos/10000^{2i/d})$$

$$PE_{(pos,2i+1)} = cos(pos/10000^{2i/d})$$

其中，pos表示单词在句子中的位置。d表示PE的维度（与词Embedding一致），2i表示偶数的维度，2i+1表示奇数维度（即2i<=d,2i+1<=d）。使用这种公式计算PE下面的好处：

* 使PE能够适应比训练集里面所有句子更长的句子，假设训练集里面最长的句子是有20个单词，突然来了一个长度为21的单词，使用公式计算的方法可以计算出第21位的 position embedding
* 可以让模型更容易地计算出相对位置，对于固定长度的间距K，PE(pos+k)可以用PE（pos）计算得到。
  * $$Sin(A+B) = Sin(A)cos(B) + Cos(A)Sin(B)$$
  * $$Cos(A+B) = Cos(A)Cos(B) - Sin(A)Sin(B)$$


### 2.2 自注意力机制(Self-Attention)
![](Md_img/2023-06-19-21-17-36.png)

