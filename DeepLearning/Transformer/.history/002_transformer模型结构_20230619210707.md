<!--
 * @Author: LiuFeng(USTC) : liufeng2317@mail.ustc.edu.cn
 * @Date: 2023-06-19 20:55:05
 * @LastEditors: LiuFeng
 * @LastEditTime: 2023-06-19 21:06:15
 * @FilePath: /Transformer/002_transformer模型结构.md
 * @Description: 
 * Copyright (c) 2023 by ${git_name} email: ${git_email}, All Rights Reserved.
-->
## 1. 整体结构

![](Md_img/2023-06-19-20-55-35.png)

* Encoder
* Decoder都包含6个block

### 1.1 步骤1：Embedding
将输入句子的每一个单词用向量X表示（词Embedding + 位置Embedding）
![](Md_img/2023-06-19-20-57-53.png)

### 1.2 Encoder
将单词得到的词向量矩阵传入Encoder中，经过6个Encoder Block 可以得到句子所有单词的编码信息矩阵C

![](Md_img/2023-06-19-21-01-41.png)

### 1.3 Decoder
将Encoder输出的编码信息矩阵C传递到Decoder之中，Decoder会根据当前翻译过的单词 1~i，翻译下一个单词i+1. 翻译过程中单词 i+1需要通过 **Mask** 掩盖

![](Md_img/2023-06-19-21-03-30.png)

## 2. 细节说明
### 2.1 Transformer输入
Transformer中单词的输入表示 x由**单词Embedding**和**位置Embedding**相加得到

![](Md_img/2023-06-19-21-06-01.png)

#### (1) 单词Embedding
（1）Word2Vec
（2）Glove
（3）可以再Transformer中训练得到

#### (2) 位置Embedding
