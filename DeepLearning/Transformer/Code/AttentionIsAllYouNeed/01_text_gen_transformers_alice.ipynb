{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "import matplotlib.pyplot as plt \n",
    "import math \n",
    "import tiktoken\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils.text_gen import get_batch,train,validate,NLPDataset\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader,Dataset,Subset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from utils.model_linear_decoder import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed.\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 23 10:56:37 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.86.01    Driver Version: 515.86.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:73:00.0  On |                  N/A |\n",
      "| 29%   41C    P8    27W / 250W |    748MiB / 11264MiB |     36%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1968      G   /usr/lib/xorg/Xorg                 59MiB |\n",
      "|    0   N/A  N/A      2441      G   /usr/lib/xorg/Xorg                343MiB |\n",
      "|    0   N/A  N/A      2571      G   /usr/bin/gnome-shell               45MiB |\n",
      "|    0   N/A  N/A      4045      G   ...2gtk-4.0/WebKitWebProcess        2MiB |\n",
      "|    0   N/A  N/A      6219      G   ...Tencent\\WeChat\\WeChat.exe       11MiB |\n",
      "|    0   N/A  N/A      6919      G   ...cent\\WeChat\\WeChatApp.exe       12MiB |\n",
      "|    0   N/A  N/A    349214      G   gnome-control-center                3MiB |\n",
      "|    0   N/A  N/A   1108290      G   ...nlogin/bin/sunloginclient       11MiB |\n",
      "|    0   N/A  N/A   3276227      G   ...RendererForSitePerProcess      101MiB |\n",
      "|    0   N/A  N/A   3336800    C+G   ...737778520744514058,131072      136MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = 'outputs/text_gen_simple_dec_alice' \n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alice.txt']\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = os.path.join('./data/TextGeneration/', 'alice_short_story')\n",
    "train_file = os.listdir(dataset_dir)\n",
    "print(train_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Dataset\n",
    "\n",
    "Let's find the longest review in the entire training set. As this will also contain the <br> tags, we will take the average of that.\n",
    "\n",
    "We will pad the smaller sentences to this average length and truncate the longer sentences to the average length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alice was a curious and imaginative young girl who lived in a quiet village. She had a wild mop of blonde curls that seemed to match her adventurous spirit. One sunny afternoon, while chasing a playful white rabbit through the meadow, Alice stumbled upon a hidden rabbit hole. Without a second thought, she decided to follow the rabbit, tumbling headfirst into an enchanting world called Wonderland.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.path.join(dataset_dir,train_file[0])\n",
    "f = open(file_path)\n",
    "lines = f.readlines()\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words (possibly, without tokenization): 1243 words\n"
     ]
    }
   ],
   "source": [
    "def find_longest_length(text_file_paths):\n",
    "    \"\"\"\n",
    "    Find the longest review length in the entire training set. \n",
    "\n",
    "    :param text_file_paths: List, containing all the text file paths.\n",
    "\n",
    "    Returns:\n",
    "        max_len: Longest review length.\n",
    "    \"\"\"\n",
    "    max_length = 0\n",
    "    for path in text_file_paths:\n",
    "        with open(path, 'r') as f:\n",
    "            text = f.read()\n",
    "            corpus = [\n",
    "                word for word in text.split()\n",
    "            ]\n",
    "        if len(corpus) > max_length:\n",
    "            max_length = len(corpus)\n",
    "    return max_length\n",
    "\n",
    "file_paths = []\n",
    "file_paths.extend(glob.glob(os.path.join(\n",
    "    dataset_dir, '*.txt'\n",
    ")))\n",
    "longest_sentence_length = find_longest_length(file_paths)\n",
    "print(f\"Total words (possibly, without tokenization): {longest_sentence_length} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words to generate in a sentence.\n",
    "SEQUENCE_LENGTH = 128\n",
    "# Vocabulary size.\n",
    "NUM_WORDS = 50304\n",
    "\n",
    "# Batch size.\n",
    "BATCH_SIZE = 1\n",
    "VALID_SPLIT = 0.1\n",
    "EPOCHS = 500"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "a few helper functions to prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_frequency(text_file_paths,num_files,most_common=None):\n",
    "    \"\"\"\n",
    "    Create a list of tuples of the following format,\n",
    "    [('ho', 2), ('hello', 1), (\"let's\", 1), ('go', 1)]\n",
    "    where the number represents the frequency of occurance of \n",
    "    the word in the entire dataset.\n",
    "\n",
    "    :param text_file_paths: List, containing all the text file paths.\n",
    "    :param most_common: Return these many top words from the dataset.\n",
    "        If `most_common` is None, return all. If `most_common` is 3,\n",
    "        returns the top 3 tuple pairs in the list.\n",
    "\n",
    "    Returns:\n",
    "        sorted_words: A list of tuple containing each word and it's\n",
    "        frequency of the format ('ho', 2), ('hello', 1), ...]\n",
    "    \"\"\"\n",
    "    # add all the words in the entire dataset to 'corpus' list\n",
    "    corpus = []\n",
    "    for i,path in enumerate(text_file_paths):\n",
    "        if i+1 == num_files:\n",
    "            break\n",
    "        with open(path,'r') as f:\n",
    "            text = f.read()\n",
    "            # remove <br> tags\n",
    "            text = re.sub('<[^>]+>+','',text)\n",
    "            corpus.extend([word for word in text.split()])\n",
    "        \n",
    "    count_words = Counter(corpus)\n",
    "    # Create a dictionary with the most common word in the corpus \n",
    "    # at the beginning.\n",
    "    # `word_frequency` will be like \n",
    "    word_frequency = count_words.most_common(n=most_common) # Returns all as n is `None`.\n",
    "    return word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2int(input_words,num_words):\n",
    "    \"\"\"\n",
    "    Create a dictionary of word to integer mapping for each unique word.\n",
    "\n",
    "    :param input_words: A list of tuples containing the words and \n",
    "        theiry frequency. Should be of the following format,\n",
    "        [('ho', 2), ('hello', 1), (\"let's\", 1), ('go', 1)]\n",
    "    :param num_words: Number of words to use from the `input_words` list \n",
    "        to create the mapping. If -1, use all words in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        int_mapping: A dictionary of word and a integer mapping as \n",
    "            key-value pair. Example, {'Hello,': 1, 'the': 2, 'let': 3}\n",
    "    \"\"\"\n",
    "    if num_words>-1:\n",
    "        int_mapping = {\n",
    "            w:i+1 for i,(w,c) in enumerate(input_words) \\\n",
    "                if i<=num_words-1 # -1 to avoid getting (num_words + 1) integer mapping.\n",
    "        }\n",
    "    else:\n",
    "        int_mapping = {w:i+1 for i, (w, c) in enumerate(input_words)}\n",
    "    return int_mapping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'gpt2'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = tiktoken.encoding_for_model(\"gpt2\")\n",
    "enc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare PyTorch Datasets and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: torch.Size([1536])\n",
      "Number of unique tokens: 618\n"
     ]
    }
   ],
   "source": [
    "file_paths = os.path.join(dataset_dir, train_file[0])\n",
    "\n",
    "dataset_inst = NLPDataset(file_paths, enc)\n",
    "dataset = dataset_inst.get_data()\n",
    "\n",
    "print(f\"Total tokens: {dataset.shape}\")\n",
    "print(f\"Number of unique tokens: {len(np.unique(dataset))}\")\n",
    "# print(f\"Number of chosen words to act as vocabulary (tokens): {len(int_mapping)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 1383\n",
      "Number of validation samples: 153\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "# Calculate the validation dataset size.\n",
    "valid_size = int(VALID_SPLIT*dataset_size)\n",
    "# Radomize the data indices.\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "# Training and validation sets.\n",
    "dataset_train = dataset[:-valid_size]\n",
    "dataset_valid = dataset[-valid_size:]\n",
    "\n",
    "print(f\"Number of training samples: {len(dataset_train)}\")\n",
    "print(f\"Number of validation samples: {len(dataset_valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    embed_dim=512, \n",
    "    src_vocab_size=NUM_WORDS, \n",
    "    seq_len=SEQUENCE_LENGTH,\n",
    "    num_layers=6, \n",
    "    expansion_factor=4, \n",
    "    n_heads=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embedding): Embedding(\n",
      "      (embed): Embedding(50304, 512)\n",
      "    )\n",
      "    (positional_encoding): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=50304, bias=True)\n",
      ")\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "65,822,976 total parameters.\n",
      "65,822,976 training parameters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=0.0001,\n",
    ")\n",
    "\n",
    "# StepLR every specific number of epochs.\n",
    "scheduler = StepLR(\n",
    "    optimizer, \n",
    "    step_size=5, \n",
    "    gamma=0.5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = EPOCHS\n",
    "# Lists to keep track of losses and accuracies.\n",
    "train_loss, valid_loss = [], []\n",
    "train_acc, valid_acc = [], []\n",
    "pbar = tqdm(range(epochs))\n",
    "# Start the training.\n",
    "for epoch in pbar:\n",
    "    # print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss = train(\n",
    "        model, \n",
    "        dataset_train, \n",
    "        optimizer, \n",
    "        criterion,\n",
    "        SEQUENCE_LENGTH,\n",
    "        NUM_WORDS,\n",
    "        BATCH_SIZE,\n",
    "        device\n",
    "    )\n",
    "    valid_epoch_loss = validate(\n",
    "        model, \n",
    "        dataset_valid,  \n",
    "        criterion,\n",
    "        SEQUENCE_LENGTH,\n",
    "        NUM_WORDS,\n",
    "        BATCH_SIZE,\n",
    "        device\n",
    "    )\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    valid_loss.append(valid_epoch_loss)\n",
    "    pbar.set_description(\"[INFO]: Epoch {},Training loss: {},Validation loss: {}\".format(epoch+1,train_epoch_loss,valid_epoch_loss))\n",
    "    # print(f\"Training loss: {train_epoch_loss}\")\n",
    "    # print(f\"Validation loss: {valid_epoch_loss}\")\n",
    "\n",
    "    # Save model.\n",
    "    torch.save(\n",
    "        model, os.path.join(CHECKPOINT_DIR, 'model.pth')\n",
    "    )\n",
    "    # print('-'*50)\n",
    "#     if epoch + 1 <= 32:\n",
    "#         scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(train_acc, valid_acc, train_loss, valid_loss):\n",
    "    \"\"\"\n",
    "    Function to save the loss and accuracy plots to disk.\n",
    "    \"\"\"\n",
    "    plt.show()\n",
    "    # Loss plots.\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(\n",
    "        train_loss, color='blue', linestyle='-', \n",
    "        label='train loss'\n",
    "    )\n",
    "    plt.plot(\n",
    "        valid_loss, color='red', linestyle='-', \n",
    "        label='validataion loss'\n",
    "    )\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "#     plt.savefig(f\"../outputs/loss.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plots(train_acc, valid_acc, train_loss, valid_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = torch.load(\n",
    "    os.path.join(CHECKPOINT_DIR, 'model.pth')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_int_vector(enc, text):\n",
    "    \"\"\"\n",
    "        Assign an integer to each word and return the integers in a list.\n",
    "    \"\"\"\n",
    "    return enc.encode(text)\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt2\")\n",
    "\n",
    "def sample_next(predictions, temperature=1.0):\n",
    "    \"\"\"\n",
    "        Implement variable-temperature sampling from a probability\n",
    "        distribution.\n",
    "    \"\"\"\n",
    "    predictions = predictions.squeeze(0)[-1, :] / temperature\n",
    "    predictions = predictions.exp().cpu()\n",
    "    next_token = torch.multinomial(predictions, num_samples=1)\n",
    "    return int(next_token[0].cpu())\n",
    "    \n",
    "\n",
    "def text_generator(sentence, generate_length):\n",
    "    trained_model.eval()\n",
    "    temperatures = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] \n",
    "    num_tokens = len(return_int_vector(enc, sentence))\n",
    "    for temeperature in temperatures:\n",
    "        sample = sentence\n",
    "        print(f\"GENERATED SENTENCE WITH TEMPERATURE {temeperature}\")\n",
    "        for i in range(generate_length-num_tokens):\n",
    "            int_vector = return_int_vector(enc, sample)\n",
    "            input_tensor = torch.tensor(int_vector, dtype=torch.int32)\n",
    "            input_tensor = input_tensor.unsqueeze(0).to(device)\n",
    "            # print(input_tensor.shape)\n",
    "            with torch.no_grad():\n",
    "                predictions = trained_model(input_tensor)\n",
    "            next_token = sample_next(predictions)\n",
    "#             if next_token != 0: # Ignore <pad> index. Final sentence may be shorter.\n",
    "            sample = sample + enc.decode([next_token])\n",
    "        print('#'*50)\n",
    "        print(sample)\n",
    "        print('#'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Alice was a curious and imaginative young girl who lived in a quiet village.\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.1\n",
      "##################################################\n",
      "Alice was a curious and imaginative young girl who lived in a quiet village. She traveled through the world called Wonderland began to influence and shape the rabbit hole and wonder to follow the rabbit hole, and distort.\n",
      "\n",
      " of the realm of the rabbit hole, Alice fell through the world. Determined to follow the world.\n",
      "\n",
      " of the most extraordinary dream world.\n",
      " Her determination led her extraordinary journeys and sought out individuals with a timeless reminder to cross paths with recapturing the rabbit hole and embarked on a quest to uncover the rabbit down the elusive rabbit down the lingering magic of her dream, Alice approached life with lead to uncover the secrets of\n",
      "##################################################\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.2\n",
      "##################################################\n",
      "Alice was a curious and imaginative young girl who lived in a quiet village. They explained that Wonderland was not just a revealed to influence and imaginative-on and shape the fabric of her consciousness. They explained that she possessed a Dream Weavers revealed to influence and reality intertwined.\n",
      "\n",
      " of purpose, inspiration, Alice embraced her consciousness. As she stepped through countless dreamscape, her experiences in Wonderland was not just a sense of dreamers across the fabric of dreamers across the dreams were not just a sense of dreamers across the depths of purpose, Alice embraced her experiences in Wonderland began to those in Wonderland began to those in Wonderland began to influence\n",
      "##################################################\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.3\n",
      "##################################################\n",
      "Alice was a curious and imaginative young girl who lived in a quiet village. As she explored this shared dreamscape, and sparkling meadows. She uncovered buried memories, Alice also stumbled upon a realm where dreams and unresolved conflicts, which she had to face head-on and unearthed hidden depths of her lucid dreaming exploration, she possessed a mischievous sprite who guided her childhood innocence, she never knew she never knew she never knew she had to face head-on and sparkling meadows.\n",
      "\n",
      " of her childhood innocence, Alice encountered her dreams and reality intertwined.\n",
      "\n",
      " of her childhood innocence, Alice also stumbled upon a mischievous sprite\n",
      "##################################################\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.4\n",
      "##################################################\n",
      "Alice was a curious and imaginative young girl who lived in a quiet village. Intrigued, ethereal beings tasked with golden light. Here, a rare gift—a profound connection to Alice that Wonderland was not just a rare gift—a profound connection to the fabric of blonde curls that Wonderland was not just a tangible world she met a rare gift—a profound connection to Alice that Wonderland began to the Dream Weavers, shimmering caterpillar who guided her role as a tangible world accessible through, she possessed a sense of the meadows. As she met a mysterious door, Alice that she met a mere dream but a tangible world accessible through\n",
      "##################################################\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.5\n",
      "##################################################\n",
      "Alice was a curious and imaginative young girl who lived in a quiet village. Here, finding herself in a realm where dreams and shape the world accessible through the world called Wonderland began to those in Wonderland began to Alice that Wonderland.\n",
      "As Alice grew in Wonderland was not just a group of dreams.\n",
      "Filled with a Dream Weaver spread, leaving behind traces of her role as a quiethemeral illusions but a sense of magic and wonder to influence and shape the lives of magic and shape the Dream Weaver spread, inspiration, hosted a sense of purpose, leaving behind traces of magic and unlock their creative potential.\n",
      "\n",
      " of dreamers across the\n",
      "##################################################\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.6\n",
      "##################################################\n",
      "Alice was a curious and imaginative young girl who lived in a quiet village. But her consciousness.\n",
      "The Dream Weavers, she stepped through, she possessed a realm where dreams and reality intertwined.\n",
      "Through her stepped through, and sparkling meadows. Here, finding herself in a mysterious door, she finally landed, and reality intertwined.\n",
      "\n",
      " of dreams of her lucid dreaming exploration, finding herself in a mysterious door, and reality intertwined.\n",
      "\n",
      " of dreams, Alice that Wonderland. As she possessed a realm within her fears personified as towering monsters, Alice fell through, ethereal beings tasked with doors of her childhood innocence, shimmer grop\n",
      "##################################################\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.7\n",
      "##################################################\n",
      "Alice was a curious and imaginative young girl who lived in a quiet village.\n",
      "\n",
      " of dreams. As she possessed a group of dreams and shape the depths of dreams and wonder to self-discovery and shape the lives of others.\n",
      "\n",
      " of dreamscapes, using her consciousness. As she explored this shared dream world accessible through countless dreamscapes, and wonder to artists and more people conquer their nightmares, inspiration, touching the lives of dreamers across the realm of dreamers across the lives of dreamscapes, and compassion. From troubled souls seeking solace to those in the lives of dreams were not just ephemeral illusions\n",
      "##################################################\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.8\n",
      "##################################################\n",
      "Alice was a curious and imaginative young girl who lived in a quiet village. They explained that Wonderland was not just a rare gift—a profound connection to those in a tangible world accessible through, she possessed a mere dreamscape, and reality intertwined. As she stepped through the had to the depths of her consciousness.\n",
      "\n",
      " of her experiences in a group of Dream Weavers, she possessed a quiet traveled through the meadows.\n",
      "Filled with a sense of purpose, finding herself in Wonderland began to those in need.\n",
      "\n",
      " of Dream Realm. They explained that she explored this shared dreamscape, she found herself in Wonderland began to face\n",
      "##################################################\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.9\n",
      "##################################################\n",
      "Alice was a curious and imaginative young girl who lived in a quiet village.\n",
      "\n",
      " of others. Here, her consciousness. As she met a realm where dreams and reality intertwined. As she met a mere dreamscape, shimmering with golden light.\n",
      "\n",
      " of dreams. She traveled through, shimmering with golden light.\n",
      "The Dream Realm. As she explored this shared dreamscapes, leaving behind traces of dreams.\n",
      "\n",
      " of others. She traveled through countless dreamers across the depths of dreams. They explained that Wonderland began to the fabric of dreams of dreams.\n",
      "With the world accessible through the lives of magic and shape the\n",
      "##################################################\n",
      "GENERATED SENTENCE WITH TEMPERATURE 1.0\n",
      "##################################################\n",
      "Alice was a curious and imaginative young girl who lived in a quiet village. As she stepped through the depths of her consciousness. Here, finding herself in a playful white rabbit through the world accessible through, she found herself in a kaleidoscope of colors and shapes. She had to match her pursuit of her surprise, Alice approached a peculiar room filled with doors of all sizes. As she were floating in a mere dreamers across the world accessible through the world.\n",
      "\n",
      " of the rabbit down the rabbit hole, the world. Determined to twist and distort. From that seemed to twist and wonder to follow the world. When she were floating\n",
      "##################################################\n",
      "\n",
      "############\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_length = 128\n",
    "\n",
    "sentences = [\n",
    "    \"Alice was a curious and imaginative young girl who lived in a quiet village.\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(f\"PROMPT: {sentence}\")\n",
    "    text_generator(sentence, generate_length)\n",
    "    print('\\n############\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoCot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c657fdcd87c2f7704702095968f689ef6d7c26f8bdf2c649392001040a57a883"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
